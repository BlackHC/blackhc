### Hi there üëã

I have recently finished my PhD ("DPhil" with [AIMS CDT](https://aims.robots.ox.ac.uk/)) in Machine Learning at [OATML](https://oatml.cs.ox.ac.uk/) (at the University of Oxford). Here is my quick online CV ü§ó

[![https://www.blackhc.net](https://img.shields.io/badge/www.blackhc.net-%231DA1F2.svg?style=flat)](https://www.blackhc.net)</button>
[![@blackhc](https://img.shields.io/badge/@blackhc-%231DA1F2.svg?style=social&logo=Twitter)](https://twitter.com/@blackhc)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-%230A66C2.svg?style=flat&logo=LinkedIn&logoColor=white)](https://linkedin.com/in/blackhc)
[![Google Scholar](https://img.shields.io/badge/Google%20Scholar-%234285F4.svg?style=flat&logo=googlescholar&logoColor=white)](https://scholar.google.com/citations?user=WYQVZpYAAAAJ)

------

<details markdown>
<summary markdown>
  
## üéì Education & üíº Industry Experience

</summary>

1. ### DPhil Computer Science
   **University of Oxford**, supervised by Prof Yarin Gal, _Oxford, UK, Oct 2018 -- Summer 2023__  
   Deep active learning and data subset selection using information theory and Bayesian neural networks.

1. ### Research Engineer (Intern)
   **Opal Camera**, _Remote, Oct 2022 -- Dec 2022_  
   Validation Pipeline for Gesture Control System.

2. ### Resident Fellow
   **Newspeak House**, _London, UK, Jan 2018 -- Jul 2018_   
   AI & Politics event series, science communication.

3. ### Performance Research Engineer
   **DeepMind**, _London, UK, Oct 2016 -- Aug 2017_  
   TensorFlow performance improvements (custom CUDA kernels) & profiling (i.a. ‚Äú[Neural Episodic Control](https://arxiv.org/abs/1703.01988)‚Äù); automated agent regression testing.

4. ### Software Engineer
   **Google**, _ZuÃàrich, CH, Jul 2013 -- Sep 2016_  
   App & testing infrastructure; latency optimization; front-end development (Dart/GWT).

3. ### MSc Computer Science
   **Technische Universit√§t MuÃànchen**, _MuÃànchen, DE, Sep 2009 -- Oct 2012_  
   Thesis ‚Äú[Assisted Object Placement](http://blog.blackhc.net/projects/university/msc-thesis-assisted-object-placement/)‚Äù.

4. ### BSc Mathematics
   **Technische Universit√§t MuÃànchen**, _MuÃànchen, DE, Sep 2009 -- Mar 2012_  
   Thesis ‚Äú[Discrete Elastic Rods](http://blog.blackhc.net/projects/university/bsc-thesis-discrete-elastic-rods/)‚Äù.

5. ### BSc Computer Science
   **Technische Universit√§t MuÃànchen**, _MuÃànchen, DE, Sep 2007 -- Sep 2009_  
   Thesis ‚Äú[Multi-Tile Terrain Rendering with OGL/Equalizer](http://stuff.blackhc.net/publications/cg_bsc_thesis.pdf)‚Äù.
</details>
<details>
<summary>
  
## üßë‚Äçüî¨ Research

</summary>

### üìö Publications

#### Conference Proceedings

[\[1\]](https://arxiv.org/abs/2102.11582) J. Mukhoti<sup>\*</sup>, **A.
Kirsch**<sup>\*</sup>, J. van Amersfoort, P. H. Torr, and Y. Gal, "*Deterministic
Neural Networks with Appropriate Inductive Biases Capture Epistemic and
Aleatoric Uncertainty*," CVPR 2023, 2023.

[\[2\]](https://arxiv.org/abs/2304.08151) F. Bickford Smith<sup>\*</sup>, **A.
Kirsch**<sup>\*</sup>, S. Farquhar, Y. Gal, A. Foster, and T. Rainforth,
"*Prediction-Oriented Bayesian Active Learning*," AISTATS, 2023.

[\[3\]](https://proceedings.mlr.press/v162/mindermann22a.) S.
Mindermann<sup>\*</sup>, J. M. Brauner<sup>\*</sup>, M. T. Razzak<sup>\*</sup>, **A. Kirsch**, et al.,
"*Prioritized Training on Points that are Learnable, Worth Learning, and
not yet Learnt*," ICML, 2022.

[\[4\]](https://arxiv.org/abs/2111.02275) A. Jesson<sup>\*</sup>, P. Tigas<sup>\*</sup>,
J. van Amersfoort, **A. Kirsch**, U. Shalit, and Y. Gal, "*Causal-BALD:
Deep Bayesian Active Learning of Outcomes to Infer Treatment-Effects
from Observational Data*," NeurIPS, 2021.

[\[5\]](https://arxiv.org/abs/1906.08158) **A. Kirsch**<sup>\*</sup>, J. van
Amersfoort<sup>\*</sup>, and Y. Gal, "*BatchBALD: Efficient and Diverse Batch
Acquisition for Deep Bayesian Active Learning*," NeurIPS, 2019.

#### Journal Articles

[\[6\]](https://arxiv.org/abs/2302.08981) **A. Kirsch**, "*Black-Box Batch Active Learning for Regression*", TMLR, 2023.

[\[7\]](https://arxiv.org/abs/2303.14753) **A. Kirsch**, "*Does ‚ÄòDeep Learning on a Data Diet‚Äô reproduce? Overall yes, but GraNd at Initialization does not*", TMLR, 2023.

[\[8\]](https://arxiv.org/abs/2106.12059) **A. Kirsch**<sup>\*</sup>, S. Farquhar<sup>\*</sup>, P. Atighehchian, A. Jesson, F. Branchaud-Charron, Y. Gal, "*Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning*", TMLR, 2023.

[\[9\]](https://arxiv.org/abs/2202.01851) **A. Kirsch** and Y. Gal, "*A
Note on "Assessing Generalization of SGD via Disagreement"*," TMLR,
2022.

[\[10\]](https://arxiv.org/abs/2208.00549) **A. Kirsch** and Y. Gal,
"*Unifying Approaches in Data Subset Selection via Fisher Information
and Information-Theoretic Quantities*," TMLR, 2022.

#### Workshop Papers

[\[11\]](https://openreview.net/forum?id=6x0gB9gOHFg) D. Tran, J. Liu, M.
W. Dusenberry, et al., "*Plex: Towards Reliability using Pretrained
Large Model Extensions*," Principles of Distribution Shifts & First
Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward,
ICML 2022.

[\[12\]](https://arxiv.org/abs/2205.08766) **A. Kirsch**, J. Kossen, and
Y. Gal, "*Marginal and Joint Cross-Entropies & Predictives for Online
Bayesian Inference, Active Learning, and Active Sampling*," Updatable
Machine Learning, ICML 2022, 2022.

[\[13\]](http://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-092.pdf)
**A. Kirsch**, J. Mukhoti, J. van Amersfoort, P. H. Torr, and Y. Gal,
"*On Pitfalls in OoD Detection: Entropy Considered Harmful*,"
Uncertainty in Deep Learning, 2021.

[\[14\]](https://arxiv.org/abs/2106.11719) **A. Kirsch**, T. Rainforth,
and Y. Gal, "*Active Learning under Pool Set Distribution Shift and
Noisy Data*," SubSetML, 2021.

[\[15\]](https://arxiv.org/abs/2106.12059) **A. Kirsch**<sup>\*</sup>, S.
Farquhar<sup>\*</sup>, and Y. Gal, "*A Simple Baseline for Batch Active Learning
with Stochastic Acquisition Functions*," SubSetML, 2021.

[\[16\]](https://arxiv.org/abs/2106.12062) **A. Kirsch** and Y. Gal, "*A
Practical & Unified Notation for Information-Theoretic Quantities in
ML*," SubSetML, 2021.

[\[17\]](http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-019.pdf)
**A. Kirsch**, C. Lyle, and Y. Gal, "*Scalable Training with Information
Bottleneck Objectives*," Uncertainty in Deep Learning,

[\[18\]](http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-075.pdf)
**A. Kirsch**, C. Lyle, and Y. Gal, "*Learning CIFAR-10 with a Simple
Entropy Estimator Using Information Bottleneck Objectives*," Uncertainty
in Deep Learning, 2020.

-----

### üìù Reviewing
NeurIPS 2019 (Top Reviewer), AAAI 2020, AAAI 2021, ICLR 2021, NeurIPS 2021 (Outstanding Reviewer), NeurIPS 2022, NeurIPS 2022, TMLR, CVPR 2023.

</details>

-----

## üéØ Interests & Skills
Active Learning, Subset Selection, Information Theory, Information Bottlenecks, Uncertainty Quantification, Python, PyTorch, Jax, C++, CUDA, TensorFlow.
</details>

-----
